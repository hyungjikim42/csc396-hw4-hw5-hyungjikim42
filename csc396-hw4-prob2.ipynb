{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a652130-1273-4b11-9815-10510b3ccd93",
   "metadata": {},
   "source": [
    "Implement the most_similar() function from the chapter 9 code, and use it to run the six examples in the notebook. Include the output of these calls in your notebook.\n",
    " \n",
    "IMPORTANT NOTE: the most_similar() function operates over actual words, whereas the embeddings you computed in problem 1 operate over transformer tokens. That is, each English word may consist of one or more tokens. To aggregate token embeddings into word embeddings, implement the following algorithm:\n",
    "1. Take the glove_vocabulary.txt file and tokenize all the words in this file using the same tokenizer you used in the previous problem.\n",
    "2. Compute a word embedding for all words in this file by averaging the corresponding token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a7fb54-d67d-453b-aee0-679b3d5585a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 2024\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 2024\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6390608-a9eb-4b59-866b-83696b4ac434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "transformer_name=\"Tejas3/distillbert_base_uncased_80_equal\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(transformer_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name, use_fast=True)\n",
    "model = AutoModel.from_pretrained(transformer_name, config=config)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb2808c-c97e-4160-ba0a-cc36a7c59da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word\n",
       "0              the\n",
       "1                ,\n",
       "2                .\n",
       "3               of\n",
       "4               to\n",
       "...            ...\n",
       "399995      chanty\n",
       "399996      kronik\n",
       "399997     rolonda\n",
       "399998     zsombor\n",
       "399999  sandberger\n",
       "\n",
       "[400000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'glove.6B.300d-vocabulary.txt'\n",
    "\n",
    "vocabs = open(file).read().splitlines()\n",
    "df = pd.DataFrame(vocabs, columns=['word'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d1c8d3-8c6b-49d1-ba8a-a865c90da990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    word: Dataset({\n",
       "        features: ['word'],\n",
       "        num_rows: 400000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['word'] = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4300cac6-bde9-4c81-bc37-1e3390cd2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):    \n",
    "    return tokenizer(batch['word'], return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d8420f-e4be-46f4-812b-5ab5c6b1b2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1cc6a3eee74e54b42dcc2c53a8e68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = ds['word'].map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ce0a92-7a86-4d27-98da-cbd12bed0888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5200e3bf-9e1f-4d12-a2d1-34bdc36292d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>[101, 1996, 102, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>[101, 1010, 102, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>[101, 1012, 102, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>[101, 1997, 102, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>[101, 2000, 102, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty</td>\n",
       "      <td>[101, 16883, 2100, 102, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik</td>\n",
       "      <td>[101, 1047, 4948, 5480, 102, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda</td>\n",
       "      <td>[101, 20996, 7811, 2850, 102, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor</td>\n",
       "      <td>[101, 1062, 25426, 12821, 102, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger</td>\n",
       "      <td>[101, 5472, 14859, 102, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                         input_ids  \\\n",
       "0              the                         [101, 1996, 102, 0, 0, 0]   \n",
       "1                ,                         [101, 1010, 102, 0, 0, 0]   \n",
       "2                .                         [101, 1012, 102, 0, 0, 0]   \n",
       "3               of                         [101, 1997, 102, 0, 0, 0]   \n",
       "4               to                         [101, 2000, 102, 0, 0, 0]   \n",
       "...            ...                                               ...   \n",
       "399995      chanty      [101, 16883, 2100, 102, 0, 0, 0, 0, 0, 0, 0]   \n",
       "399996      kronik    [101, 1047, 4948, 5480, 102, 0, 0, 0, 0, 0, 0]   \n",
       "399997     rolonda   [101, 20996, 7811, 2850, 102, 0, 0, 0, 0, 0, 0]   \n",
       "399998     zsombor  [101, 1062, 25426, 12821, 102, 0, 0, 0, 0, 0, 0]   \n",
       "399999  sandberger      [101, 5472, 14859, 102, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                           attention_mask  \n",
       "0                      [1, 1, 1, 0, 0, 0]  \n",
       "1                      [1, 1, 1, 0, 0, 0]  \n",
       "2                      [1, 1, 1, 0, 0, 0]  \n",
       "3                      [1, 1, 1, 0, 0, 0]  \n",
       "4                      [1, 1, 1, 0, 0, 0]  \n",
       "...                                   ...  \n",
       "399995  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "399996  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  \n",
       "399997  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  \n",
       "399998  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]  \n",
       "399999  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[400000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e78621ae-1f81-48e9-8706-56df67f4fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "word_embeddings = []\n",
    "\n",
    "def process(batch):\n",
    "    tokenized_word = tokenizer(batch['word'], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_word)\n",
    "\n",
    "    # extract embedding for the batch\n",
    "    hidden_state = output.last_hidden_state\n",
    "    # word embedding = average of its contexutalized token embeddings\n",
    "    word_emb = hidden_state[0].mean(dim=0)\n",
    "\n",
    "    # normalize\n",
    "    word_emb = normalize(word_emb, p=2, dim=0)\n",
    "\n",
    "    # map word to its corresponding embedding\n",
    "    word = batch['word']\n",
    "    # word_embeddings[word] = word_emb\n",
    "    word_embeddings.append(word_emb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07761097-2a20-4bf0-9f1b-479fc79093d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51804/2319920495.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  word_embeddings = torch.load(\"word_embeddingsss.pt\")\n"
     ]
    }
   ],
   "source": [
    "ds.map(process)\n",
    "\n",
    "# word_embeddings is an array of tensors--use torch.stack to create tensor of tensors\n",
    "word_embeddings = torch.stack(word_embeddings)\n",
    "\n",
    "# save as output file to avoid running it multiple times :(\n",
    "# torch.save(word_embeddings, \"word_embeddingsss.pt\")\n",
    "# word_embeddings = torch.load(\"word_embeddingsss.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b69f876c-8242-4864-aea2-657a7d680214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to numpy array\n",
    "word_embeddings = word_embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03275b00-2a46-40e6-88ee-9e777a306ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_key = {}\n",
    "key_to_index = {}\n",
    "\n",
    "for i, row in enumerate(ds['word']):\n",
    "    word = row['word']\n",
    "    index_to_key[i] = word\n",
    "    key_to_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4117b68c-8958-420e-b779-acc51ba45ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def most_similar(word,topn=10):\n",
    "    word_id = key_to_index[word]\n",
    "    emb = word_embeddings[word_id]\n",
    "    similarities = word_embeddings @ emb\n",
    "    ids_ascending = similarities.argsort()\n",
    "    ids_descending = ids_ascending[::-1]\n",
    "    mask = ids_descending != word_id\n",
    "    # obtain new array of indices that doesn't contain the word itself\n",
    "    ids_descending = ids_descending[mask]\n",
    "    top_ids = ids_descending[:topn]\n",
    "    top_words = [(index_to_key[i], similarities[i]) for i in top_ids]\n",
    "    return top_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24cd7853-313b-4617-93b4-f218a9e3cb3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cactuses', 0.91599166),\n",
       " ('pear', 0.9150654),\n",
       " ('spineflower', 0.910865),\n",
       " ('juniper', 0.90248454),\n",
       " ('shrub', 0.9016392),\n",
       " ('acacia', 0.8984697),\n",
       " ('shrubs', 0.89702),\n",
       " ('orchid', 0.89677685),\n",
       " ('pinaster', 0.8966114),\n",
       " ('berries', 0.8954787)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8029fd25-4357-4a51-b6a8-630cfc2b2b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cakes', 0.94355893),\n",
       " ('cakebread', 0.92435),\n",
       " ('dessert', 0.9212095),\n",
       " ('pattycake', 0.90699303),\n",
       " ('cupcake', 0.904183),\n",
       " ('mooncake', 0.90096116),\n",
       " ('mooncakes', 0.8971993),\n",
       " ('desserts', 0.8965075),\n",
       " ('cakewalk', 0.8943194),\n",
       " ('pancakes', 0.89414)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('cake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc9af062-d867-4d7f-bc86-f1f7e1f641e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frustrated', 0.94961965),\n",
       " ('enraged', 0.94812036),\n",
       " ('agitated', 0.9381641),\n",
       " ('irritated', 0.9356283),\n",
       " ('annoyed', 0.92857295),\n",
       " ('furious', 0.9282503),\n",
       " ('angering', 0.9281014),\n",
       " ('impatient', 0.9263562),\n",
       " ('ugly', 0.9241725),\n",
       " ('complaining', 0.92405325)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd8f4f6-3c8c-438d-a45c-74a8a003f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('immediately', 0.89024985),\n",
       " ('rapidly', 0.8540477),\n",
       " ('swiftly', 0.84985775),\n",
       " ('quick', 0.82023597),\n",
       " ('rún', 0.81379414),\n",
       " ('run', 0.81379414),\n",
       " ('prepare', 0.8123312),\n",
       " ('ensure', 0.81138396),\n",
       " ('hurry', 0.80761504),\n",
       " ('grab', 0.80675554)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2660a1cb-24f9-4c0f-a19f-3712a3486baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('betweens', 0.8975253),\n",
       " ('in-between', 0.89026386),\n",
       " ('after', 0.8871184),\n",
       " ('thé', 0.87822294),\n",
       " ('thế', 0.87822294),\n",
       " ('the', 0.87822294),\n",
       " ('tō', 0.8777739),\n",
       " ('tổ', 0.8777739),\n",
       " ('tộ', 0.8777739),\n",
       " ('tô', 0.8777739)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "869a2aee-b930-4633-9567-2127db8642ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thé', 0.99999976),\n",
       " ('thế', 0.99999976),\n",
       " ('a', 0.93305206),\n",
       " ('à', 0.93305206),\n",
       " ('å', 0.93305206),\n",
       " ('ą', 0.93305206),\n",
       " ('á', 0.93305206),\n",
       " ('ã', 0.93305206),\n",
       " ('ă', 0.93305206),\n",
       " ('ä', 0.93305206)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4c5db-2d60-43d9-8466-ee275517681d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc2a4e-5a37-4ee4-b0cb-001d1923e8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfc2f8-dce9-44a3-bb12-a0a2df13f03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fa19e-2f8e-460c-a4c2-c2f6a012cfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e41881-b614-4b1d-8d31-8dd0e779c9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20d4457d-1b2c-4d3f-b747-e1450e01bd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('starving', 0.9208219),\n",
       " ('thirsty', 0.91829467),\n",
       " ('feral', 0.89709157),\n",
       " ('desperate', 0.8959713),\n",
       " ('starved', 0.88797283),\n",
       " ('rotting', 0.8857002),\n",
       " ('impatient', 0.8796371),\n",
       " ('sick', 0.8785124),\n",
       " ('unwilling', 0.8770848),\n",
       " ('needy', 0.87336016)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"hungry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a52a987f-2629-483c-8bb1-ddc477a1892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frustrated', 0.94961965),\n",
       " ('enraged', 0.94812036),\n",
       " ('agitated', 0.9381641),\n",
       " ('irritated', 0.9356283),\n",
       " ('annoyed', 0.92857295),\n",
       " ('furious', 0.9282503),\n",
       " ('angering', 0.9281014),\n",
       " ('impatient', 0.9263562),\n",
       " ('ugly', 0.9241725),\n",
       " ('complaining', 0.92405325)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c55702c-062a-42ad-ba13-159e6562d474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yung', 0.9123292),\n",
       " ('kwang', 0.9063474),\n",
       " ('seang', 0.9040199),\n",
       " ('jongg', 0.8955262),\n",
       " ('chayng', 0.89248216),\n",
       " ('boonsong', 0.89116025),\n",
       " ('jeng', 0.8898934),\n",
       " ('deliang', 0.8889395),\n",
       " ('leong', 0.8877564),\n",
       " ('alving', 0.88761723)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"ryang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ce813fc-aff9-4ff0-86c3-a739b90b7133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.92228246),\n",
       " ('monarch', 0.9065795),\n",
       " ('queenan', 0.89665383),\n",
       " ('princess', 0.89159346),\n",
       " ('consort', 0.88092244),\n",
       " ('princessa', 0.8798687),\n",
       " ('kings', 0.8774574),\n",
       " ('kinga', 0.8680315),\n",
       " ('queen-consort', 0.86543465),\n",
       " ('queening', 0.86539793)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e45f5cd-2966-4c7b-a3ae-f1445673fdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.92228246),\n",
       " ('kinga', 0.9129591),\n",
       " ('kings', 0.908782),\n",
       " ('kingii', 0.87946886),\n",
       " ('monarch', 0.8785908),\n",
       " ('kinglets', 0.8720195),\n",
       " ('kingo', 0.86991453),\n",
       " ('kingsize', 0.86771184),\n",
       " ('prince', 0.8656887),\n",
       " ('queenan', 0.8655345)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9037ab0-2496-4e02-85e8-5e646fbfce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.93490076),\n",
       " ('princes', 0.92972887),\n",
       " ('princesa', 0.88537294),\n",
       " ('princelings', 0.88534784),\n",
       " ('princessa', 0.8757585),\n",
       " ('king', 0.8656887),\n",
       " ('princeling', 0.86075485),\n",
       " ('babys', 0.8541628),\n",
       " ('baby', 0.85275245),\n",
       " ('beau', 0.8513527)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"prince\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
