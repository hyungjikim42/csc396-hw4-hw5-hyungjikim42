# csc396-hw4-hw5-hyungjikim42

To run the notebooks, make sure the required data files are placed in the same directory as the notebook files.

## HW4
For Problem 1, due to Cyverse constantly freezing, I used only a subset of the original dataset instead of the full dataset.

## HW5

The implementation for HW5 was largely based on the Hugging Face tutorial (("https://huggingface.co/learn/nlp-course/en/chapter7/7")).
I did have to make several modifications during the process and as a result, I am not entirely confident that the model is working correctly. I may have misunderstood some part of it.

Due to limited resources, I used only a small portion of the original dataset for this assignment as well. Additionally, I'm uncertain whether I sliced the data correctly. Despite all these issues, the model yielded an EM score of 0.40 and an F1 score of 0.65, which seems unusually high. It's possible that something went wrong during the data preprocessing or evaluation process, but it could also be that the BERT model performs really well even with a small amount of data!

It would be fun to further explore...
